# RAG (Retrieval-Augmented Generation) - LangChain

![LangChain](img/langchain.jpeg)

Les syst√®mes **RAG (Retrieval-Augmented Generation)** dans LangChain permettent aux mod√®les de langage de s'appuyer sur des **connaissances externes** pour produire des r√©ponses plus pr√©cises, actualis√©es et pertinentes.

Contrairement √† un simple LLM qui g√©n√®re une r√©ponse uniquement √† partir de ce qu'il a appris pendant son entra√Ænement, un syst√®me RAG interroge une base de documents pour retrouver des morceaux d'information pertinents ‚Äì appel√©s **chunks** ‚Äì et les injecte dans le prompt du LLM.

![RAG](img/rag.jpeg)

**Que montre le sch√©ma ci-dessus ?**

Le processus se divise en **deux grandes phases** : **pr√©paration des documents** et **traitement des requ√™tes**.

**Pr√©paration des documents (√† gauche)**
- (1) Un fichier (document source) est divis√© en **chunks**, c'est-√†-dire en petits segments de texte.
- (2) Chaque chunk est pass√© dans un LLM Embedder, un encodeur qui transforme le texte en un vecteur num√©rique (**embeddings**).
- (3) Ces vecteurs sont ensuite stock√©s dans un Vector Store, une base de donn√©es sp√©cialis√©e pour les recherches par **similarit√© s√©mantique**.

**Traitement des requ√™tes (√† droite)**
- (a) Lorsqu'un utilisateur emet une requ√™te, celle-ci est √† son tour encod√©e via **le m√™me LLM Embedder** pour obtenir son vecteur.
- (b) Ce vecteur est utilis√© par le **Retriever**, qui compare la requ√™te aux vecteurs des **chunks** pour trouver les plus similaires.
- (c) Les chunks retrouv√©s sont envoy√©s au LLM, qui les utilise comme contexte pour formuler une r√©ponse.


En r√©sum√©, ce fonctionnement est illustr√© par la boucle :

> Requ√™te ‚Üí Encodage ‚Üí Recherche dans la base vectorielle ‚Üí R√©cup√©ration des chunks ‚Üí Passage au LLM ‚Üí R√©ponse contextuelle

## 1. Chargement du mod√®le LLM local

Dans cette section, nous chargeons un mod√®le de langage local gr√¢ce √† **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion √† une API externe.

Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d'interagir facilement avec un mod√®le comme **llama3** ainsi qu'un **mod√®le d'embeddings** d√©j√† t√©l√©charg√©s via Ollama.

## 2. RAG standard

Le **RAG standard** consiste √† :
- formuler une requ√™te explicite
- interroger une base de documents vectoris√©e
- utiliser un mod√®le LLM pour g√©n√©rer une r√©ponse √† partir des r√©sultats retrouv√©s.

Ce pipeline est **efficace pour des questions ind√©pendantes, sans contexte conversationnel**.

### 2.1 Pr√©paration des documents

Nous initialisons les chemins n√©cessaires √† la pr√©paration des documents d'entr√©e.

### 2.2 Initialisation du vector store

Nous v√©rifions ici si la base vectorielle existe d√©j√†.
Si ce n'est pas le cas, le fichier source est charg√©, d√©coup√© en morceaux, enrichi de m√©tadonn√©es, puis index√© dans Chroma DB.

### 2.3 Initialisation du moteur de recherche vectorielle

Une fois la base vectorielle Chroma initialis√©e avec les embeddings, nous la transformons en **moteur de recherche (retriever)**.
Cela permet de retrouver les documents les plus proches s√©mantiquement d'une question ou d'une requ√™te.

### 2.4 Ex√©cution d'une requ√™te de recherche

Dans cette √©tape, nous combinons la recherche vectorielle avec un LLM.
L'objectif est de fournir une r√©ponse pertinente √† une question, en s'appuyant uniquement sur les documents retrouv√©s dans la base vectorielle.
Le mod√®le est guid√© par un prompt structur√© qui inclut la requ√™te initiale et les contenus des chunks pertinents.

### üß© Exercice

La soci√©t√© NovTech g√®re de nombreux documents internes :
- des rapports d'incidents (panne, erreur technique, post-mortem),
- des proc√©dures op√©rationnelles (onboarding, acc√®s syst√®me, d√©ploiement‚Ä¶).

Actuellement, les √©quipes perdent du temps √† chercher les bonnes informations √† travers des fichiers √©parpill√©s.

Votre objectif est de construire un assistant bas√© sur l'architecture RAG qui permettra :
- de retrouver rapidement les proc√©dures en cas de besoin,
- de consulter les r√©solutions d'incidents similaires,
- de r√©pondre √† des questions en langage naturel en s'appuyant uniquement sur les documents internes.

Pour vous aider, vous pouvez suivre les √©tapes suivantes :
1. Chargement des documents
2. D√©coupage en chunks
3. Indexation vectorielle
4. Recherche contextuelle
5. G√©n√©ration de r√©ponse

‚ÑπÔ∏è Les documents de l'entreprise se trouve dans le dossier `data/novtech`.
üí™üèª **Bonus** : Rendre possible un filtrage par cat√©gorie dans les recherches

## 3. RAG conversationnel

Dans un cadre d'**interaction continue**, les utilisateurs posent souvent des questions implicites ou r√©f√©rentielles (ex. "Et lui ?"). Le **RAG conversationnel** ajoute une √©tape cl√© : la **reformulation de la question en prenant en compte l'historique du dialogue**.

Cette version de RAG permet de maintenir la pertinence des recherches dans la base vectorielle tout en conservant la fluidit√© de la conversation, ce qui la rend adapt√©e aux assistants IA ou aux chatbots avanc√©s.

**Exemple**

Historique de la conversation :
- Utilisateur : *Qui est le CEO de Tesla ?*
- IA : *Elon Musk est le CEO de Tesla*.
- Utilisateur : *Et de SpaceX ?*

‚û°Ô∏è La question "Et de SpaceX ?" est ambigu√´ seule. Le moteur de recherche (retriever) ne sait pas de quoi il s'agit exactement.

Avec une reformulation de la question de l'utilisateur cela donnerait : "Qui est le CEO de SpaceX ?"

‚û°Ô∏è R√©sultat : la requ√™te est claire, et la recherche dans la base vectorielle peut retourner les bons documents.

**üëç LangChain facilite ce processus**

LangChain fournit une abstraction pr√™te √† l'emploi gr√¢ce √† la classe `ConversationalRetrievalChain`.
Cette classe prend automatiquement en charge :
- la reformulation de la question via le LLM
- la recherche dans la base vectorielle
- la g√©n√©ration de la r√©ponse finale √† partir des documents r√©cup√©r√©s et de l'historique

‚û°Ô∏è Elle encapsule ainsi toute la logique conversationnelle d'un RAG en une seule ligne.

### üß© Exercice

Repartez de l'exercice pr√©c√©dent (NovTech), et impl√©mentez un assistant de conversation continue.

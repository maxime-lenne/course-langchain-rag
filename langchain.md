# RAG (Retrieval-Augmented Generation) - LangChain

![LangChain](img/langchain.jpeg)

Les systÃ¨mes **RAG (Retrieval-Augmented Generation)** dans LangChain permettent aux modÃ¨les de
langage de s'appuyer sur des **connaissances externes** pour produire des rÃ©ponses plus prÃ©cises,
actualisÃ©es et pertinentes.

Contrairement Ã  un simple LLM qui gÃ©nÃ¨re une rÃ©ponse uniquement Ã  partir de ce qu'il a appris
pendant son entraÃ®nement, un systÃ¨me RAG interroge une base de documents pour retrouver des
morceaux d'information pertinents â€“ appelÃ©s **chunks** â€“ et les injecte dans le prompt du LLM.

![RAG](img/rag.jpeg)

**Que montre le schÃ©ma ci-dessus ?**

Le processus se divise en **deux grandes phases** : **prÃ©paration des documents** et **traitement
des requÃªtes**.

**PrÃ©paration des documents (Ã  gauche)**

- (1) Un fichier (document source) est divisÃ© en **chunks**, c'est-Ã -dire en petits segments
  de texte.
- (2) Chaque chunk est passÃ© dans un LLM Embedder, un encodeur qui transforme le texte en un
  vecteur numÃ©rique (**embeddings**).
- (3) Ces vecteurs sont ensuite stockÃ©s dans un Vector Store, une base de donnÃ©es spÃ©cialisÃ©e
  pour les recherches par **similaritÃ© sÃ©mantique**.

**Traitement des requÃªtes (Ã  droite)**

- (a) Lorsqu'un utilisateur emet une requÃªte, celle-ci est Ã  son tour encodÃ©e via
  **le mÃªme LLM Embedder** pour obtenir son vecteur.
- (b) Ce vecteur est utilisÃ© par le **Retriever**, qui compare la requÃªte aux vecteurs des
  **chunks** pour trouver les plus similaires.
- (c) Les chunks retrouvÃ©s sont envoyÃ©s au LLM, qui les utilise comme contexte pour formuler
  une rÃ©ponse.

En rÃ©sumÃ©, ce fonctionnement est illustrÃ© par la boucle :

> RequÃªte â†’ Encodage â†’ Recherche dans la base vectorielle â†’ RÃ©cupÃ©ration des chunks â†’
> Passage au LLM â†’ RÃ©ponse contextuelle

## 1. Chargement du modÃ¨le LLM local

Dans cette section, nous chargeons un modÃ¨le de langage local grÃ¢ce Ã  **Ollama**. Cela permet
de travailler avec un **LLM directement sur notre machine**, sans connexion Ã  une API externe.

Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d'interagir
facilement avec un modÃ¨le comme **llama3** ainsi qu'un **modÃ¨le d'embeddings** dÃ©jÃ  tÃ©lÃ©chargÃ©s
via Ollama.

> **Import Chroma (LangChain v1)** : utiliser `from langchain_chroma import Chroma`
> (`pip install langchain-chroma`) Ã  la place de l'ancien
> `from langchain_community.vectorstores import Chroma`.

## 2. RAG standard

Le **RAG standard** consiste Ã  :

- formuler une requÃªte explicite
- interroger une base de documents vectorisÃ©e
- utiliser un modÃ¨le LLM pour gÃ©nÃ©rer une rÃ©ponse Ã  partir des rÃ©sultats retrouvÃ©s.

Ce pipeline est **efficace pour des questions indÃ©pendantes, sans contexte conversationnel**.

### 2.1 PrÃ©paration des documents

Nous initialisons les chemins nÃ©cessaires Ã  la prÃ©paration des documents d'entrÃ©e.

### 2.2 Initialisation du vector store

Nous vÃ©rifions ici si la base vectorielle existe dÃ©jÃ .
Si ce n'est pas le cas, le fichier source est chargÃ©, dÃ©coupÃ© en morceaux, enrichi de
mÃ©tadonnÃ©es, puis indexÃ© dans Chroma DB.

### 2.3 Initialisation du moteur de recherche vectorielle

Une fois la base vectorielle Chroma initialisÃ©e avec les embeddings, nous la transformons en
**moteur de recherche (retriever)**.
Cela permet de retrouver les documents les plus proches sÃ©mantiquement d'une question ou d'une
requÃªte.

### 2.4 ExÃ©cution d'une requÃªte de recherche

Le pipeline est composÃ© de 4 Ã©tapes enchaÃ®nÃ©es avec l'opÃ©rateur `|` (LCEL) :

1. **RÃ©cupÃ©ration** : `retriever | format_docs` cherche les chunks pertinents et les formate
   en texte brut.
2. **Prompt** : `ChatPromptTemplate` injecte le contexte documentaire et la question dans un
   template structurÃ©.
3. **LLM** : le modÃ¨le gÃ©nÃ¨re une rÃ©ponse Ã  partir du prompt enrichi.
4. **Parser** : `StrOutputParser()` extrait le texte de la rÃ©ponse (plus besoin de `.content`).

```python
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)
```

### ğŸ§© Exercice

La sociÃ©tÃ© NovTech gÃ¨re de nombreux documents internes :

- des rapports d'incidents (panne, erreur technique, post-mortem),
- des procÃ©dures opÃ©rationnelles (onboarding, accÃ¨s systÃ¨me, dÃ©ploiementâ€¦).

Actuellement, les Ã©quipes perdent du temps Ã  chercher les bonnes informations Ã  travers des
fichiers Ã©parpillÃ©s.

Votre objectif est de construire un assistant basÃ© sur l'architecture RAG qui permettra :

- de retrouver rapidement les procÃ©dures en cas de besoin,
- de consulter les rÃ©solutions d'incidents similaires,
- de rÃ©pondre Ã  des questions en langage naturel en s'appuyant uniquement sur les documents
  internes.

Pour vous aider, vous pouvez suivre les Ã©tapes suivantes :

1. Chargement des documents
2. DÃ©coupage en chunks
3. Indexation vectorielle
4. Recherche contextuelle
5. GÃ©nÃ©ration de rÃ©ponse

â„¹ï¸ Les documents de l'entreprise se trouve dans le dossier `data/novtech`.
ğŸ’ªğŸ» **Bonus** : Rendre possible un filtrage par catÃ©gorie dans les recherches

## 3. RAG conversationnel

Dans un cadre d'**interaction continue**, les utilisateurs posent souvent des questions
implicites ou rÃ©fÃ©rentielles (ex. "Et lui ?"). Le **RAG conversationnel** maintient un historique
de la conversation pour permettre ces questions de suivi.

**Approche LCEL avec historique explicite :**

On utilise un `MessagesPlaceholder` dans le prompt pour injecter `chat_history` (liste de
`HumanMessage` / `AIMessage`) Ã  chaque tour. La liste est mise Ã  jour manuellement aprÃ¨s chaque
Ã©change.

```python
prompt_conv = ChatPromptTemplate.from_messages([
    ("system", "..."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "Documents pertinents :\n\n{context}\n\nQuestion : {question}")
])

conversational_rag_chain = (
    {
        "context": lambda x: format_docs(retriever.invoke(x["question"])),
        "question": lambda x: x["question"],
        "chat_history": lambda x: x["chat_history"]
    }
    | prompt_conv
    | model
    | StrOutputParser()
)
```

> ğŸ’¡ Pour une mÃ©moire persistante multi-sessions (avec `MemorySaver` et `thread_id`),
> voir `langgraph.ipynb`.

### ğŸ§© Exercice

Repartez de l'exercice prÃ©cÃ©dent (NovTech), et implÃ©mentez un assistant de conversation
continue avec la chaÃ®ne LCEL + `MessagesPlaceholder`.

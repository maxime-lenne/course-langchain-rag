import marimo

__generated_with = "0.10.0"
app = marimo.App(width="medium")


@app.cell
def __():
    import marimo as mo
    return (mo,)


@app.cell
def __(mo):
    mo.md(
        """
        ![LangChain](img/langchain.jpeg)

        # RAG (Retrieval-Augmented Generation) avec LangChain

        Les syst√®mes **RAG** permettent aux mod√®les de langage de s'appuyer sur des
        **connaissances externes** pour produire des r√©ponses plus pr√©cises et pertinentes.

        Contrairement √† un simple LLM, un syst√®me RAG interroge une base de documents
        pour retrouver des morceaux d'information pertinents ‚Äî appel√©s **chunks** ‚Äî
        et les injecte dans le prompt du LLM.
        """
    )
    return


@app.cell
def __(mo):
    mo.md(
        """
        ![RAG](img/rag.jpeg)

        ## Processus RAG

        **Pr√©paration des documents :**
        1. Document ‚Üí **Chunks** (d√©coupage)
        2. Chunks ‚Üí **Embeddings** (vectorisation)
        3. Embeddings ‚Üí **Vector Store** (stockage)

        **Traitement des requ√™tes :**
        1. Requ√™te ‚Üí **Embedding**
        2. Recherche par **similarit√©** dans le Vector Store
        3. Chunks pertinents + Requ√™te ‚Üí **LLM** ‚Üí R√©ponse
        """
    )
    return


@app.cell
def __(mo):
    mo.md("# 1. Chargement du mod√®le LLM et Embedder")
    return


@app.cell
def __():
    import os
    from dotenv import load_dotenv
    from langchain_ollama import ChatOllama, OllamaEmbeddings
    from langchain_core.messages import HumanMessage, AIMessage
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain_core.runnables import RunnablePassthrough
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.document_loaders import TextLoader
    from langchain_chroma import Chroma  # ‚úÖ LangChain v1 : pip install langchain-chroma

    load_dotenv(override=True)

    model = ChatOllama(model="llama3", temperature=0)
    embedder = OllamaEmbeddings(model="nomic-embed-text")
    return (
        os, load_dotenv, model, embedder,
        ChatOllama, OllamaEmbeddings,
        HumanMessage, AIMessage,
        StrOutputParser, ChatPromptTemplate, MessagesPlaceholder, RunnablePassthrough,
        RecursiveCharacterTextSplitter, TextLoader, Chroma,
    )


@app.cell
def __(mo):
    mo.md(
        """
        # 2. RAG standard

        Le RAG standard consiste √† :
        - Formuler une requ√™te explicite
        - Interroger une base de documents vectoris√©e
        - Utiliser un LLM pour g√©n√©rer une r√©ponse

        Ce pipeline est efficace pour des **questions ind√©pendantes, sans contexte conversationnel**.
        """
    )
    return


@app.cell
def __(mo):
    mo.md("### 2.1 Pr√©paration des documents")
    return


@app.cell
def __(os):
    # Chemins vers les fichiers
    current_dir = os.getcwd()
    file_name = "meeting_reports.txt"
    file_path = os.path.join(current_dir, "data", file_name)
    db_dir = os.path.join(current_dir, "data", "db")
    return current_dir, file_name, file_path, db_dir


@app.cell
def __(mo):
    mo.md("### 2.2 Initialisation du vector store")
    return


@app.cell
def __(os, mo, file_path, db_dir, embedder, TextLoader, RecursiveCharacterTextSplitter, Chroma):
    # V√©rification si la base existe d√©j√†
    if not os.path.exists(db_dir):
        mo.md("**Initialisation du vector store...**")

        # Chargement du document
        loader = TextLoader(file_path)
        loaded_document = loader.load()

        # D√©coupage en chunks
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
        chunks = text_splitter.split_documents(loaded_document)

        # Ajout de m√©tadonn√©es
        for chunk in chunks:
            chunk.metadata["source"] = file_path
            chunk.metadata["category"] = "meeting"

        # Cr√©ation de la base vectorielle
        db = Chroma.from_documents(chunks, embedder, persist_directory=db_dir)
        mo.md("**Vector store cr√©√© !**")
    else:
        mo.md("**Vector store existant charg√©.**")
    return


@app.cell
def __(mo):
    mo.md("### 2.3 Initialisation du moteur de recherche")
    return


@app.cell
def __(db_dir, embedder, Chroma):
    # Chargement de la base vectorielle
    db = Chroma(persist_directory=db_dir, embedding_function=embedder)

    # Conversion en retriever
    retriever = db.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}
    )
    return db, retriever


@app.cell
def __(mo):
    mo.md(
        """
        ### 2.4 Ex√©cution d'une requ√™te de recherche

        Le pipeline est compos√© de 4 √©tapes encha√Æn√©es avec l'op√©rateur `|` :
        1. **R√©cup√©ration** : le retriever cherche les chunks pertinents et une fonction les formate
        2. **Prompt** : le contexte et la question sont inject√©s dans un template structur√©
        3. **LLM** : le mod√®le g√©n√®re une r√©ponse √† partir du prompt enrichi
        4. **Parser** : la sortie brute est convertie en cha√Æne de caract√®res simple
        """
    )
    return


@app.cell
def __(mo):
    # Interface utilisateur pour la requ√™te
    query_input = mo.ui.text(
        value="Quels sont les r√©unions concernant la soci√©t√© Neolink ?",
        placeholder="Entrez votre question...",
        label="Question"
    )
    query_input
    return (query_input,)


@app.cell
def __(mo, model, retriever, query_input, ChatPromptTemplate, RunnablePassthrough, StrOutputParser):
    if query_input.value:
        # Fonction de formatage des documents
        def _format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        # Template du prompt
        _prompt = ChatPromptTemplate.from_messages([
            ("system",
             "Tu es un assistant qui aide √† retrouver tout type d'informations interne √† une entreprise. "
             "R√©ponds uniquement en te basant sur les documents fournis. "
             "Si l'information n'est pas dans les documents, dis-le clairement."),
            ("human", "Documents pertinents :\n\n{context}\n\nQuestion : {question}")
        ])

        # Cha√Æne RAG avec LCEL
        rag_chain = (
            {"context": retriever | _format_docs, "question": RunnablePassthrough()}
            | _prompt
            | model
            | StrOutputParser()
        )

        result = rag_chain.invoke(query_input.value)

        mo.md(f"""
        **Question :** {query_input.value}

        ---

        **R√©ponse :**

        {result}
        """)
    return


@app.cell
def __(mo):
    mo.md(
        """
        ## Exercice 1

        La soci√©t√© **NovTech** g√®re de nombreux documents internes :
        - Rapports d'incidents (`data/novtech/incidents/`)
        - Proc√©dures op√©rationnelles (`data/novtech/procedures/`)

        Construisez un assistant RAG qui permet de :
        - Retrouver rapidement les proc√©dures
        - Consulter les r√©solutions d'incidents similaires
        - R√©pondre en langage naturel

        **Bonus** : Permettre un filtrage par cat√©gorie (incidents vs proc√©dures)
        """
    )
    return


@app.cell
def __(mo):
    # Votre code ici
    mo.md("Compl√©tez l'exercice ci-dessus")
    return


@app.cell
def __(mo):
    mo.md(
        """
        # 3. RAG conversationnel

        Le **RAG conversationnel** maintient un historique de la conversation pour
        permettre des questions de suivi implicites (ex. "Et lui ?").

        **Approche LCEL avec historique explicite :**
        On utilise un `MessagesPlaceholder` dans le prompt pour injecter l'historique
        (`chat_history`) √† chaque tour. La liste est mise √† jour manuellement
        apr√®s chaque √©change (`HumanMessage` / `AIMessage`).

        > üí° Pour une m√©moire persistante multi-sessions, voir `rag.ipynb`.
        """
    )
    return


@app.cell
def __(mo):
    # √âtat pour l'historique de conversation (persistant entre les re-ex√©cutions Marimo)
    get_history, set_history = mo.state([])
    return get_history, set_history


@app.cell
def __(mo):
    chat_query = mo.ui.text(
        placeholder="Posez une question...",
        label="Votre question"
    )
    chat_query
    return (chat_query,)


@app.cell
def __(mo, model, retriever, chat_query, get_history, set_history,
       ChatPromptTemplate, MessagesPlaceholder, StrOutputParser,
       HumanMessage, AIMessage):
    if chat_query.value:
        # Prompt avec historique de conversation
        _prompt_conv = ChatPromptTemplate.from_messages([
            ("system",
             "Tu es un assistant qui aide √† retrouver tout type d'informations interne √† une entreprise. "
             "R√©ponds uniquement en te basant sur les documents fournis. "
             "Si l'information n'est pas dans les documents, dis-le clairement."),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "Documents pertinents :\n\n{context}\n\nQuestion : {question}")
        ])

        # Cha√Æne RAG conversationnelle avec LCEL
        _chain = (
            {
                "context": lambda x: "\n\n".join(
                    doc.page_content for doc in retriever.invoke(x["question"])
                ),
                "question": lambda x: x["question"],
                "chat_history": lambda x: x["chat_history"]
            }
            | _prompt_conv
            | model
            | StrOutputParser()
        )

        _result = _chain.invoke({
            "question": chat_query.value,
            "chat_history": get_history()
        })

        # Mise √† jour de l'historique
        _new_history = get_history() + [
            HumanMessage(content=chat_query.value),
            AIMessage(content=_result)
        ]
        set_history(_new_history)

        mo.md(f"""
        **Vous :** {chat_query.value}

        **Assistant :** {_result}
        """)
    else:
        mo.md("*Posez une question pour commencer*")
    return


@app.cell
def __(mo):
    mo.md(
        """
        ## Exercice 2

        Repartez de l'exercice NovTech et impl√©mentez un assistant de conversation continue
        avec la cha√Æne LCEL + `MessagesPlaceholder`.

        L'assistant doit pouvoir :
        - Se souvenir des questions pr√©c√©dentes
        - Utiliser le contexte de l'historique pour des questions de suivi implicites
        """
    )
    return


@app.cell
def __(mo):
    # Votre code ici
    mo.md("Compl√©tez l'exercice ci-dessus")
    return


if __name__ == "__main__":
    app.run()

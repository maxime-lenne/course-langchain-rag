{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f4459b",
   "metadata": {},
   "source": [
    "![LangChain](img/langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511a0f0",
   "metadata": {},
   "source": [
    "Les syst√®mes **RAG (Retrieval-Augmented Generation)** dans LangChain permettent aux mod√®les de langage de s‚Äôappuyer sur des **connaissances externes** pour produire des r√©ponses plus pr√©cises, actualis√©es et pertinentes.\n",
    "\n",
    "Contrairement √† un simple LLM qui g√©n√®re une r√©ponse uniquement √† partir de ce qu‚Äôil a appris pendant son entra√Ænement, un syst√®me RAG interroge une base de documents pour retrouver des morceaux d‚Äôinformation pertinents ‚Äì appel√©s **chunks** ‚Äì et les injecte dans le prompt du LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb160db",
   "metadata": {},
   "source": [
    "![RAG](img/rag.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3075ae",
   "metadata": {},
   "source": [
    "**Que montre le sch√©ma ci-dessus ?**\n",
    "\n",
    "Le processus se divise en **deux grandes phases** : **pr√©paration des documents** et **traitement des requ√™tes**.\n",
    "\n",
    "**Pr√©paration des documents (√† gauche)**\n",
    "- (1) Un fichier (document source) est divis√© en **chunks**, c‚Äôest-√†-dire en petits segments de texte.\n",
    "- (2) Chaque chunk est pass√© dans un LLM Embedder, un encodeur qui transforme le texte en un vecteur num√©rique (**embeddings**).\n",
    "- (3) Ces vecteurs sont ensuite stock√©s dans un Vector Store, une base de donn√©es sp√©cialis√©e pour les recherches par **similarit√© s√©mantique**.\n",
    "\n",
    "**Traitement des requ√™tes (√† droite)**\n",
    "- (a) Lorsqu‚Äôun utilisateur emet une requ√™te, celle-ci est √† son tour encod√©e via **le m√™me LLM Embedder** pour obtenir son vecteur.\n",
    "- (b) Ce vecteur est utilis√© par le **Retriever**, qui compare la requ√™te aux vecteurs des **chunks** pour trouver les plus similaires.\n",
    "- (c) Les chunks retrouv√©s sont envoy√©s au LLM, qui les utilise comme contexte pour formuler une r√©ponse.\n",
    "\n",
    "\n",
    "En r√©sum√©, ce fonctionnement est illustr√© par la boucle :\n",
    "\n",
    "> Requ√™te ‚Üí Encodage ‚Üí Recherche dans la base vectorielle ‚Üí R√©cup√©ration des chunks ‚Üí Passage au LLM ‚Üí R√©ponse contextuelle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb689",
   "metadata": {},
   "source": [
    "# 1. Chargement du mod√®le\n",
    "___\n",
    "\n",
    "## LLM local :\n",
    "\n",
    "Dans cette section, nous chargeons un mod√®le de langage local gr√¢ce √† **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion √† une API externe.\n",
    "\n",
    "Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d‚Äôinteragir facilement avec un mod√®le comme llama3 d√©j√† t√©l√©charg√© via Ollama.\n",
    "\n",
    "GTP-OSS:20b, Mistral-Small3.2 24B GLM 4.7 Flash\n",
    "\n",
    "## LLM Cloud :\n",
    "Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma  # ‚úÖ LangChain v1 : pip install langchain-chroma\n",
    "\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# Chargement des cl√©s d'API se trouvant dans le fichier .env.\n",
    "# Ceci permet d'utiliser des mod√®les en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = ChatOllama(model=\"glm-4.7-flash\")\n",
    "\n",
    "#model = ChatMistralAI(model=\"mistral-large-latest\", api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "\n",
    "# Mod√®le sp√©cialis√© pour convertir du texte en vecteurs (https://ollama.com/library/nomic-embed-text).\n",
    "# Il existe d'autres mod√®les d'embeddings (comme \"all-MiniLM-L6-v2\", \"text-embedding-ada-002\", etc.)\n",
    "# avec des performances et dimensions vari√©es selon les cas d'usage (recherche s√©mantique, classification, etc.).\n",
    "embedder = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a0cd4",
   "metadata": {},
   "source": [
    "# 2. RAG standard\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3369916",
   "metadata": {},
   "source": [
    "Le **RAG standard** consiste √† :\n",
    "- formuler une requ√™te explicite\n",
    "- interroger une base de documents vectoris√©e\n",
    "- utiliser un mod√®le LLM pour g√©n√©rer une r√©ponse √† partir des r√©sultats retrouv√©s.\n",
    "\n",
    "Ce pipeline est **efficace pour des questions ind√©pendantes, sans contexte conversationnel**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7de366",
   "metadata": {},
   "source": [
    "### 2.1 Pr√©paration des documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699bd52",
   "metadata": {},
   "source": [
    "Nous initialisons les chemins n√©cessaires √† la pr√©paration des documents d‚Äôentr√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77475b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√®re le chemin absolu du r√©pertoire courant (l√† o√π le script est ex√©cut√©)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Nom du fichier texte contenant les comptes rendus de r√©union\n",
    "file_name = \"meeting_reports.txt\"\n",
    "\n",
    "# Construit le chemin complet vers le fichier texte dans le dossier \"data\"\n",
    "file_path = os.path.join(current_dir, \"data\", file_name)\n",
    "\n",
    "# D√©finit le chemin du r√©pertoire o√π sera stock√©e la base de donn√©es vectorielle (Chroma DB)\n",
    "db_dir = os.path.join(current_dir, \"data\", \"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed30a56",
   "metadata": {},
   "source": [
    "### 2.2 Initialisation du vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c526e6f",
   "metadata": {},
   "source": [
    "Nous v√©rifions ici si la base vectorielle existe d√©j√†.\n",
    "Si ce n‚Äôest pas le cas, le fichier source est charg√©, d√©coup√© en morceaux, enrichi de m√©tadonn√©es, puis index√© dans Chroma DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9884d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(db_dir):\n",
    "    print(\"Initializing vector store...\")\n",
    "\n",
    "    # Chargement du fichier texte brut contenant les documents\n",
    "    loader = TextLoader(file_path)\n",
    "    loaded_document = loader.load()\n",
    "\n",
    "    # D√©coupage du document en chunks de 1000 caract√®res avec un chevauchement de 0\n",
    "    # - chunk_size d√©termine la taille maximale de chaque morceau (en nombre de caract√®res ici : 1000)\n",
    "    # - chunk_overlap permet de conserver un chevauchement entre les morceaux pour √©viter les coupures abruptes, ici il est √† 0, donc sans recouvrement.\n",
    "    # - RecursiveCharacterTextSplitter est souvent pr√©f√©r√© en pratique pour des documents textuels comme des comptes rendus,\n",
    "    #   des articles ou de la documentation technique, car il garde mieux le contexte s√©mantique.\n",
    "    #   Ce splitter tente d'abord de d√©couper sur les sauts de ligne, puis sur les phrases, puis sur les mots, etc.\n",
    "    # ... d'autres Text Splitter comme CharachterTextSplitter existent. √Ä approfondir si besoin\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(loaded_document)\n",
    "\n",
    "    # Ajout de m√©tadonn√©es √† chaque chunk (utile pour le filtrage ou le suivi de provenance).\n",
    "    # Ici 2 metadata sont ajout√©s mais il pourrait en y avoir plus.\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"source\"] = file_path    # Chemin d'origine du document\n",
    "        chunk.metadata[\"category\"] = \"meeting\"  # Cat√©gorie de contenu (√† adapter selon les besoins)\n",
    "\n",
    "    # Cr√©ation et persistance de la base vectorielle dans le dossier d√©fini\n",
    "    db = Chroma.from_documents(chunks, embedder, persist_directory=db_dir)\n",
    "\n",
    "    print(\"Vector store created !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cbb986",
   "metadata": {},
   "source": [
    "### 2.3 Initialisation du moteur de recherche vectorielle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bc3f2",
   "metadata": {},
   "source": [
    "Une fois la base vectorielle Chroma initialis√©e avec les embeddings, nous la transformons en **moteur de recherche (retriever)**.\n",
    "Cela permet de retrouver les documents les plus proches s√©mantiquement d‚Äôune question ou d‚Äôune requ√™te."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee3ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/q0j81gmx5h7gvpg38k8hknzw0000gn/T/ipykernel_1472/573588974.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=db_dir, embedding_function=embedder)\n"
     ]
    }
   ],
   "source": [
    "# Chargement de la base vectorielle existante, avec liaison avec le m√™me embedder ayant servi pour cr√©er la base vectorielle\n",
    "db = Chroma(persist_directory=db_dir, embedding_function=embedder)\n",
    "\n",
    "# Conversion de la base Chroma en \"retriever\" pour effectuer des recherches par similarit√©\n",
    "# - search_type=\"similarity\" utilise la distance cosinus entre les vecteurs\n",
    "# - \"k\": 3 signifie que l'on souhaite r√©cup√©rer les 3 documents les plus proches\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# üí° Il est aussi possible d‚Äôutiliser d‚Äôautres types de recherche (search_type) :\n",
    "# - \"mmr\" (Maximal Marginal Relevance) : √©quilibre entre pertinence et diversit√© des r√©sultats\n",
    "# - \"similarity_score_threshold\" : retourne uniquement les documents dont le score d√©passe un certain seuil\n",
    "#      search_kwargs={\"score_threshold\": 0.8} permet par exemple de filtrer les r√©sultats peu pertinents\n",
    "#\n",
    "# D‚Äôautres param√®tres utiles dans search_kwargs :\n",
    "# - \"fetch_k\" : nombre de documents √† r√©cup√©rer avant le tri final (utile avec MMR)\n",
    "# - \"lambda_mult\" : pond√©ration entre pertinence et diversit√© dans MMR\n",
    "#\n",
    "# Etc... √† approfondir si besoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3318104",
   "metadata": {},
   "source": [
    "### 2.4 Ex√©cution d‚Äôune requ√™te de recherche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3186eb",
   "metadata": {},
   "source": "Dans cette √©tape, nous combinons la recherche vectorielle avec un LLM via une cha√Æne LCEL (LangChain Expression Language).\n\nLe pipeline est compos√© de 4 √©tapes encha√Æn√©es avec l'op√©rateur `|` :\n1. **R√©cup√©ration** : le retriever cherche les chunks pertinents et une fonction les formate en texte\n2. **Prompt** : le contexte r√©cup√©r√© et la question sont inject√©s dans un template structur√©\n3. **LLM** : le mod√®le g√©n√®re une r√©ponse √† partir du prompt enrichi\n4. **Parser** : la sortie brute du mod√®le est convertie en cha√Æne de caract√®res simple\n\nCette approche remplace le passage manuel des messages et garantit que le mod√®le r√©pond **uniquement √† partir des documents fournis**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f69c21",
   "metadata": {},
   "outputs": [],
   "source": "# Requ√™te pos√©e par l'utilisateur\nquery = \"Quels sont les r√©unions concernant la soci√©t√© Neolink ?\"\n\n# Optionnel : affichage manuel des chunks retrouv√©s (utile pour debug ou v√©rification)\n# relevant_chunks = retriever.invoke(query)\n# for i, chunk in enumerate(relevant_chunks, 1):\n#     print(f\"Chunk {i}:\\n{chunk.page_content}\\n\")\n\n# Fonction utilitaire pour formater les documents r√©cup√©r√©s en un seul bloc de texte\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# Template du prompt : le contexte r√©cup√©r√© et la question sont inject√©s dynamiquement\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\",\n     \"Tu es un assistant qui aide √† retrouver tout type d'informations interne √† une entreprise. \"\n     \"R√©ponds uniquement en te basant sur les documents fournis. \"\n     \"Si l'information n'est pas dans les documents, dis-le clairement.\"),\n    (\"human\", \"Documents pertinents :\\n\\n{context}\\n\\nQuestion : {question}\")\n])\n\n# Construction de la cha√Æne RAG avec LCEL :\n# 1. retriever | format_docs : r√©cup√®re les chunks et les formate en texte\n# 2. RunnablePassthrough() : laisse passer la question telle quelle vers le prompt\n# 3. prompt : injecte contexte + question dans le template\n# 4. model : g√©n√®re la r√©ponse\n# 5. StrOutputParser() : extrait le texte de la r√©ponse (plus besoin de .content)\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\n# Ex√©cution de la cha√Æne\nresult = rag_chain.invoke(query)\n\ndisplay(Markdown(result))"
  },
  {
   "cell_type": "markdown",
   "id": "7c362eac",
   "metadata": {},
   "source": [
    "### üß© Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ccbc1",
   "metadata": {},
   "source": [
    "La soci√©t√© NovTech g√®re de nombreux documents internes :\n",
    "- des rapports d‚Äôincidents (panne, erreur technique, post-mortem),\n",
    "- des proc√©dures op√©rationnelles (onboarding, acc√®s syst√®me, d√©ploiement‚Ä¶).\n",
    "\n",
    "Actuellement, les √©quipes perdent du temps √† chercher les bonnes informations √† travers des fichiers √©parpill√©s.\n",
    "\n",
    "Votre objectif est de construire un assistant bas√© sur l'architecture RAG qui permettra :\n",
    "- de retrouver rapidement les proc√©dures en cas de besoin,\n",
    "- de consulter les r√©solutions d‚Äôincidents similaires,\n",
    "- de r√©pondre √† des questions en langage naturel en s‚Äôappuyant uniquement sur les documents internes.\n",
    "\n",
    "Pour vous aider, vous pouvez suivre les √©tapes suivantes :\n",
    "1. Chargement des documents\n",
    "2. D√©coupage en chunks\n",
    "3. Indexation vectorielle\n",
    "4. Recherche contextuelle\n",
    "5. G√©n√©ration de r√©ponse\n",
    "\n",
    "‚ÑπÔ∏è Les documents de l'entreprise se trouve dans le dossier `data/novtech`.\n",
    "üí™üèª **Bonus** : Rendre possible un filtrage par cat√©gorie dans les recherches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49ae5cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a000884",
   "metadata": {},
   "source": [
    "# 3. RAG conversationnel\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843e3e1",
   "metadata": {},
   "source": "Dans un cadre d'**interaction continue**, les utilisateurs posent souvent des questions implicites ou r√©f√©rentielles (ex. \"Et lui ?\"). Le **RAG conversationnel** ajoute une √©tape cl√© : la **reformulation de la question en prenant en compte l'historique du dialogue**.\n\nCette version de RAG permet de maintenir la pertinence des recherches dans la base vectorielle tout en conservant la fluidit√© de la conversation, ce qui la rend adapt√©e aux assistants IA ou aux chatbots avanc√©s.\n\n**Exemple**\n\nHistorique de la conversation :\n- Utilisateur : *Qui est le CEO de Tesla ?*\n- IA : *Elon Musk est le CEO de Tesla*.\n- Utilisateur : *Et de SpaceX ?*\n\n‚û°Ô∏è La question \"Et de SpaceX ?\" est ambigu√´ seule. Le moteur de recherche (retriever) ne sait pas de quoi il s'agit exactement.\n\nAvec une reformulation de la question de l'utilisateur cela donnerait : \"Qui est le CEO de SpaceX ?\"\n\n‚û°Ô∏è R√©sultat : la requ√™te est claire, et la recherche dans la base vectorielle peut retourner les bons documents.\n\n**üëç Approche LCEL avec historique explicite**\n\nOn √©tend la cha√Æne RAG standard en lui passant un `chat_history` (liste de `HumanMessage` / `AIMessage`) via un `MessagesPlaceholder`.\n√Ä chaque tour, le mod√®le dispose √† la fois du contexte documentaire r√©cup√©r√© **et** de l'historique de la conversation, ce qui lui permet de reformuler et de r√©pondre de fa√ßon coh√©rente.\n\nLa gestion de la m√©moire reste sous le contr√¥le du d√©veloppeur : simple liste Python mise √† jour apr√®s chaque √©change.\n\n> üí° Pour aller plus loin avec une m√©moire persistante multi-sessions, voir la **section 4** de ce notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b0361",
   "metadata": {},
   "outputs": [],
   "source": "# Prompt avec historique de conversation int√©gr√© via MessagesPlaceholder\n# Le mod√®le re√ßoit : le contexte documentaire + l'historique + la nouvelle question\nprompt_conv = ChatPromptTemplate.from_messages([\n    (\"system\",\n     \"Tu es un assistant qui aide √† retrouver tout type d'informations interne √† une entreprise. \"\n     \"R√©ponds uniquement en te basant sur les documents fournis. \"\n     \"Si l'information n'est pas dans les documents, dis-le clairement.\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"human\", \"Documents pertinents :\\n\\n{context}\\n\\nQuestion : {question}\")\n])\n\n# Cha√Æne RAG conversationnelle avec LCEL\n# Le retriever est appel√© avec la question courante √† chaque tour\nconversational_rag_chain = (\n    {\n        \"context\": lambda x: \"\\n\\n\".join(doc.page_content for doc in retriever.invoke(x[\"question\"])),\n        \"question\": lambda x: x[\"question\"],\n        \"chat_history\": lambda x: x[\"chat_history\"]\n    }\n    | prompt_conv\n    | model\n    | StrOutputParser()\n)\n\n# Historique de conversation (liste de HumanMessage / AIMessage)\nchat_history = []\n\n# Boucle de chat\n# ‚ö†Ô∏è Changer `while False:` en `while True:` pour activer l'exemple\nwhile False:\n    user_input = input(\"Vous : \")\n    clear_output(wait=True)\n    display(Markdown(f\"**Vous :** {user_input}\"))\n\n    if user_input.lower() in [\"stop\", \"exit\", \"quit\"]:\n        print(\"Fin de la conversation.\")\n        break\n\n    result = conversational_rag_chain.invoke({\n        \"question\": user_input,\n        \"chat_history\": chat_history\n    })\n\n    display(Markdown(result))\n\n    # Mise √† jour de l'historique pour le prochain tour\n    chat_history.append(HumanMessage(content=user_input))\n    chat_history.append(AIMessage(content=result))"
  },
  {
   "cell_type": "markdown",
   "id": "e2a604a4",
   "metadata": {},
   "source": [
    "### üß© Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3873b2",
   "metadata": {},
   "source": [
    "Repartez de l'exercice pr√©c√©dent (NovTech), et impl√©mentez un assistant de conversation continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dy4mwgodvn",
   "source": "# 4. RAG avec m√©moire persistante (LangGraph)\n___",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "l57tyabu2qi",
   "source": "**LangGraph** est le framework d'orchestration de LangChain pour construire des applications **stateful** (avec √©tat persistant). L√† o√π la section 3 g√®re l'historique de conversation manuellement via une liste Python, LangGraph prend en charge cette m√©moire de fa√ßon **automatique, persistante et multi-sessions**.\n\nDans cette section, nous allons enrichir le RAG conversationnel avec :\n- un **agent** capable de d√©cider dynamiquement quand consulter la base documentaire,\n- une **m√©moire persistante** (`MemorySaver`) isol√©e par session via un `thread_id`,\n- la possibilit√© de g√©rer **plusieurs conversations en parall√®le** sans collision d'historique.\n\n### Pourquoi LangGraph ?\n\nDans la section 3, l'historique est une liste Python g√©r√©e manuellement :\n```python\nchat_history.append(HumanMessage(...))\nchat_history.append(AIMessage(...))\n```\nC'est suffisant pour une session simple, mais cette approche a des limites :\n- L'historique **dispara√Æt** √† la fin du processus,\n- Impossible de g√©rer **plusieurs sessions en parall√®le** sans collision,\n- Pas de possibilit√© de **reprendre** une conversation interrompue.\n\nLangGraph r√©sout ces probl√®mes avec `MemorySaver` et le concept de `thread_id`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jxtxj78shs9",
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "p35gflbxv5",
   "source": "### 4.1 D√©finition de l'outil de retrieval\n\nL'outil est une fonction d√©cor√©e avec `@tool` que l'agent peut appeler dynamiquement.\nLe d√©corateur `response_format=\"content_and_artifact\"` permet de retourner √† la fois\nle texte format√© (utilis√© dans le prompt) et les documents bruts (pour le debug).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4szbo9apay",
   "source": "@tool(response_format=\"content_and_artifact\")\ndef retrieve(query: str):\n    \"\"\"Recherche des informations dans la base de documents internes de l'entreprise.\"\"\"\n    retrieved_docs = retriever.invoke(query)\n    serialized = \"\\n\\n\".join(\n        f\"Source : {doc.metadata.get('source', 'inconnue')}\\nContenu : {doc.page_content}\"\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "h1xy9vl35nj",
   "source": "### 4.2 Cr√©ation de l'agent avec m√©moire persistante\n\n`MemorySaver` est un checkpoint en m√©moire vive fourni par LangGraph.\nIl associe automatiquement chaque √©change √† un `thread_id` : deux conversations\navec des `thread_id` diff√©rents ont des historiques totalement s√©par√©s.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xkm5zfd9qvn",
   "source": "# M√©moire persistante : chaque session identifi√©e par son thread_id a son propre historique\nmemory = MemorySaver()\n\n# L'agent d√©cide dynamiquement √† chaque tour s'il doit appeler l'outil de recherche\nagent = create_agent(\n    model,\n    tools=[retrieve],\n    system_prompt=(\n        \"Tu es un assistant qui aide √† retrouver tout type d'informations interne √† une entreprise. \"\n        \"Utilise l'outil de recherche pour trouver des informations pertinentes avant de r√©pondre. \"\n        \"Si l'information n'est pas dans les documents, dis-le clairement.\"\n    ),\n    checkpointer=memory\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "97g8xu3myvd",
   "source": "### 4.3 Conversation avec gestion du thread_id\n\nLe `thread_id` identifie la session. Pour d√©marrer une **nouvelle conversation**\nsans historique, changez simplement la valeur de `thread_id`.\n\nDeux threads peuvent tourner en parall√®le sans interf√©rence.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "o7bkncpzwaf",
   "source": "# Identifiant de session ‚Äî changer la valeur pour d√©marrer une nouvelle conversation\nconfig = {\"configurable\": {\"thread_id\": \"session_1\"}}\n\n# Boucle de chat\n# ‚ö†Ô∏è Changer `while False:` en `while True:` pour activer l'exemple\nwhile False:\n    user_input = input(\"Vous : \")\n    clear_output(wait=True)\n    display(Markdown(f\"**Vous :** {user_input}\"))\n\n    if user_input.lower() in [\"stop\", \"exit\", \"quit\"]:\n        print(\"Fin de la conversation.\")\n        break\n\n    # L'agent g√®re automatiquement : retrieval, historique, g√©n√©ration\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n        config=config\n    )\n\n    display(Markdown(response[\"messages\"][-1].content))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2ud3makwc9n",
   "source": "### 4.4 Sessions multiples en parall√®le\n\nAvec un seul agent et `MemorySaver`, on peut simuler deux utilisateurs distincts\ndont les historiques ne se m√©langent pas.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5vt3jfuo2lh",
   "source": "# Session utilisateur A\nconfig_a = {\"configurable\": {\"thread_id\": \"utilisateur_A\"}}\n# Session utilisateur B\nconfig_b = {\"configurable\": {\"thread_id\": \"utilisateur_B\"}}\n\n# Utilisateur A pose une premi√®re question\nresponse_a1 = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Quelles r√©unions concernent Neolink ?\"}]},\n    config=config_a\n)\nprint(\"[Session A ‚Äî Tour 1]\")\ndisplay(Markdown(response_a1[\"messages\"][-1].content))\n\n# Utilisateur B pose une question ind√©pendante\nresponse_b1 = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Quelles sont les d√©cisions prises en janvier ?\"}]},\n    config=config_b\n)\nprint(\"\\n[Session B ‚Äî Tour 1]\")\ndisplay(Markdown(response_b1[\"messages\"][-1].content))\n\n# Utilisateur A pose une question de suivi ‚Äî l'agent se souvient du contexte A\nresponse_a2 = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Et les participants √† ces r√©unions ?\"}]},\n    config=config_a\n)\nprint(\"\\n[Session A ‚Äî Tour 2 (question de suivi)]\")\ndisplay(Markdown(response_a2[\"messages\"][-1].content))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bn0vvrbcp1e",
   "source": "### üß© Exercice\n\nReprenez les documents NovTech (incidents et proc√©dures) d√©j√† utilis√©s dans les sections pr√©c√©dentes.\n\n1. Cr√©ez un outil `@tool` de retrieval pointant sur la base NovTech.\n2. Construisez un agent avec `create_agent` + `MemorySaver`.\n3. Simulez **deux sessions utilisateur** (`thread_id` diff√©rents) posant des questions sur des incidents.\n4. V√©rifiez que les contextes ne se m√©langent pas entre les sessions.\n\nüí™üèª **Bonus** : Ajoutez un second outil `retrieve_by_category(query, category)` qui filtre les r√©sultats par cat√©gorie (`incidents` ou `procedures`) en utilisant les m√©tadonn√©es Chroma.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hrj2i1fvm14",
   "source": "# Votre code ici",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

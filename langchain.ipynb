{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f4459b",
   "metadata": {},
   "source": [
    "![LangChain](img/langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511a0f0",
   "metadata": {},
   "source": [
    "Les syst√®mes **RAG (Retrieval-Augmented Generation)** dans LangChain permettent aux mod√®les de langage de s‚Äôappuyer sur des **connaissances externes** pour produire des r√©ponses plus pr√©cises, actualis√©es et pertinentes.\n",
    "\n",
    "Contrairement √† un simple LLM qui g√©n√®re une r√©ponse uniquement √† partir de ce qu‚Äôil a appris pendant son entra√Ænement, un syst√®me RAG interroge une base de documents pour retrouver des morceaux d‚Äôinformation pertinents ‚Äì appel√©s **chunks** ‚Äì et les injecte dans le prompt du LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb160db",
   "metadata": {},
   "source": [
    "![RAG](img/rag.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3075ae",
   "metadata": {},
   "source": [
    "**Que montre le sch√©ma ci-dessus ?**\n",
    "\n",
    "Le processus se divise en **deux grandes phases** : **pr√©paration des documents** et **traitement des requ√™tes**.\n",
    "\n",
    "**Pr√©paration des documents (√† gauche)**\n",
    "- (1) Un fichier (document source) est divis√© en **chunks**, c‚Äôest-√†-dire en petits segments de texte.\n",
    "- (2) Chaque chunk est pass√© dans un LLM Embedder, un encodeur qui transforme le texte en un vecteur num√©rique (**embeddings**).\n",
    "- (3) Ces vecteurs sont ensuite stock√©s dans un Vector Store, une base de donn√©es sp√©cialis√©e pour les recherches par **similarit√© s√©mantique**.\n",
    "\n",
    "**Traitement des requ√™tes (√† droite)**\n",
    "- (a) Lorsqu‚Äôun utilisateur emet une requ√™te, celle-ci est √† son tour encod√©e via **le m√™me LLM Embedder** pour obtenir son vecteur.\n",
    "- (b) Ce vecteur est utilis√© par le **Retriever**, qui compare la requ√™te aux vecteurs des **chunks** pour trouver les plus similaires.\n",
    "- (c) Les chunks retrouv√©s sont envoy√©s au LLM, qui les utilise comme contexte pour formuler une r√©ponse.\n",
    "\n",
    "\n",
    "En r√©sum√©, ce fonctionnement est illustr√© par la boucle :\n",
    "\n",
    "> Requ√™te ‚Üí Encodage ‚Üí Recherche dans la base vectorielle ‚Üí R√©cup√©ration des chunks ‚Üí Passage au LLM ‚Üí R√©ponse contextuelle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7eb01a",
   "metadata": {},
   "source": [
    "# 1. Chargement du mod√®le LLM local\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb689",
   "metadata": {},
   "source": [
    "Dans cette section, nous chargeons un mod√®le de langage local gr√¢ce √† **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion √† une API externe.\n",
    "\n",
    "Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d‚Äôinteragir facilement avec un mod√®le comme **llama3** ainsi qu'un **mod√®le d'embeddings** d√©j√† t√©l√©charg√©s via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7301c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Chargement des cl√©s d'API se trouvant dans le fichier .env.  \n",
    "# Ceci permet d'utiliser des mod√®les en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = ChatOllama(model=\"llama3\", temperature=0)\n",
    "#model = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"))\n",
    "\n",
    "# Mod√®le sp√©cialis√© pour convertir du texte en vecteurs (https://ollama.com/library/nomic-embed-text).\n",
    "# Il existe d'autres mod√®les d'embeddings (comme \"all-MiniLM-L6-v2\", \"text-embedding-ada-002\", etc.) \n",
    "# avec des performances et dimensions vari√©es selon les cas d‚Äôusage (recherche s√©mantique, classification, etc.).\n",
    "embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a0cd4",
   "metadata": {},
   "source": [
    "# 2. RAG standard\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3369916",
   "metadata": {},
   "source": [
    "Le **RAG standard** consiste √† :\n",
    "- formuler une requ√™te explicite\n",
    "- interroger une base de documents vectoris√©e\n",
    "- utiliser un mod√®le LLM pour g√©n√©rer une r√©ponse √† partir des r√©sultats retrouv√©s. \n",
    " \n",
    "Ce pipeline est **efficace pour des questions ind√©pendantes, sans contexte conversationnel**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7de366",
   "metadata": {},
   "source": [
    "### 2.1 Pr√©paration des documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699bd52",
   "metadata": {},
   "source": [
    "Nous initialisons les chemins n√©cessaires √† la pr√©paration des documents d‚Äôentr√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77475b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√®re le chemin absolu du r√©pertoire courant (l√† o√π le script est ex√©cut√©)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Nom du fichier texte contenant les comptes rendus de r√©union\n",
    "file_name = \"meeting_reports.txt\"\n",
    "\n",
    "# Construit le chemin complet vers le fichier texte dans le dossier \"data\"\n",
    "file_path = os.path.join(current_dir, \"data\", file_name)\n",
    "\n",
    "# D√©finit le chemin du r√©pertoire o√π sera stock√©e la base de donn√©es vectorielle (Chroma DB)\n",
    "db_dir = os.path.join(current_dir, \"data\", \"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed30a56",
   "metadata": {},
   "source": [
    "### 2.2 Initialisation du vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c526e6f",
   "metadata": {},
   "source": [
    "Nous v√©rifions ici si la base vectorielle existe d√©j√†.  \n",
    "Si ce n‚Äôest pas le cas, le fichier source est charg√©, d√©coup√© en morceaux, enrichi de m√©tadonn√©es, puis index√© dans Chroma DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9884d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(db_dir):\n",
    "    print(\"Initializing vector store...\")\n",
    "\n",
    "    # Chargement du fichier texte brut contenant les documents\n",
    "    loader = TextLoader(file_path)\n",
    "    loaded_document = loader.load()\n",
    "\n",
    "    # D√©coupage du document en chunks de 1000 caract√®res avec un chevauchement de 0\n",
    "    # - chunk_size d√©termine la taille maximale de chaque morceau (en nombre de caract√®res ici : 1000)\n",
    "    # - chunk_overlap permet de conserver un chevauchement entre les morceaux pour √©viter les coupures abruptes, ici il est √† 0, donc sans recouvrement.\n",
    "    # - RecursiveCharacterTextSplitter est souvent pr√©f√©r√© en pratique pour des documents textuels comme des comptes rendus, \n",
    "    #   des articles ou de la documentation technique, car il garde mieux le contexte s√©mantique.\n",
    "    #   Ce splitter tente d'abord de d√©couper sur les sauts de ligne, puis sur les phrases, puis sur les mots, etc.\n",
    "    # ... d'autres Text Splitter comme CharachterTextSplitter existent. √Ä approfondir si besoin\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(loaded_document)\n",
    "\n",
    "    # Ajout de m√©tadonn√©es √† chaque chunk (utile pour le filtrage ou le suivi de provenance).\n",
    "    # Ici 2 metadata sont ajout√©s mais il pourrait en y avoir plus.\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"source\"] = file_path    # Chemin d'origine du document\n",
    "        chunk.metadata[\"category\"] = \"meeting\"  # Cat√©gorie de contenu (√† adapter selon les besoins)\n",
    "\n",
    "    # Cr√©ation et persistance de la base vectorielle dans le dossier d√©fini\n",
    "    db = Chroma.from_documents(chunks, embedder, persist_directory=db_dir)\n",
    "\n",
    "    print(\"Vector store created !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cbb986",
   "metadata": {},
   "source": [
    "### 2.3 Initialisation du moteur de recherche vectorielle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bc3f2",
   "metadata": {},
   "source": [
    "Une fois la base vectorielle Chroma initialis√©e avec les embeddings, nous la transformons en **moteur de recherche (retriever)**.  \n",
    "Cela permet de retrouver les documents les plus proches s√©mantiquement d‚Äôune question ou d‚Äôune requ√™te.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee3ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/q0j81gmx5h7gvpg38k8hknzw0000gn/T/ipykernel_1472/573588974.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=db_dir, embedding_function=embedder)\n"
     ]
    }
   ],
   "source": [
    "# Chargement de la base vectorielle existante, avec liaison avec le m√™me embedder ayant servi pour cr√©er la base vectorielle\n",
    "db = Chroma(persist_directory=db_dir, embedding_function=embedder)\n",
    "\n",
    "# Conversion de la base Chroma en \"retriever\" pour effectuer des recherches par similarit√©\n",
    "# - search_type=\"similarity\" utilise la distance cosinus entre les vecteurs\n",
    "# - \"k\": 3 signifie que l'on souhaite r√©cup√©rer les 3 documents les plus proches\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# üí° Il est aussi possible d‚Äôutiliser d‚Äôautres types de recherche (search_type) :\n",
    "# - \"mmr\" (Maximal Marginal Relevance) : √©quilibre entre pertinence et diversit√© des r√©sultats\n",
    "# - \"similarity_score_threshold\" : retourne uniquement les documents dont le score d√©passe un certain seuil\n",
    "#      search_kwargs={\"score_threshold\": 0.8} permet par exemple de filtrer les r√©sultats peu pertinents\n",
    "#\n",
    "# D‚Äôautres param√®tres utiles dans search_kwargs :\n",
    "# - \"fetch_k\" : nombre de documents √† r√©cup√©rer avant le tri final (utile avec MMR)\n",
    "# - \"lambda_mult\" : pond√©ration entre pertinence et diversit√© dans MMR\n",
    "# \n",
    "# Etc... √† approfondir si besoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3318104",
   "metadata": {},
   "source": [
    "### 2.4 Ex√©cution d‚Äôune requ√™te de recherche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3186eb",
   "metadata": {},
   "source": [
    "Dans cette √©tape, nous combinons la recherche vectorielle avec un LLM.  \n",
    "L‚Äôobjectif est de fournir une r√©ponse pertinente √† une question, en s‚Äôappuyant uniquement sur les documents retrouv√©s dans la base vectorielle.  \n",
    "Le mod√®le est guid√© par un prompt structur√© qui inclut la requ√™te initiale et les contenus des chunks pertinents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0f69c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "En examinant les documents que vous m'avez fournis, je peux voir que les r√©unions concernant la soci√©t√© Neolink sont :\n",
       "\n",
       "* La r√©union du 09/03/2025 (Bilan des incidents techniques du mois) : Cette r√©union a port√© sur les t√¢ches r√©alis√©es et les √©l√©ments bloquants li√©s aux interactions avec NeoLink.\n",
       "* La r√©union du 10/05/2025 (R√©union hebdomadaire de coordination technique) : Cette r√©union a √©galement abord√© les enjeux li√©s √† la communication interne, jug√©e parfois insuffisante, et aux besoins en ressources.\n",
       "\n",
       "Il n'y a pas d'autres r√©unions mentionn√©es dans les documents que vous m'avez fournis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Requ√™te pos√©e par l'utilisateur\n",
    "query = \"Quels sont les r√©unions concernant la soci√©t√© Neolink ?\"\n",
    "\n",
    "# Recherche des chunks vectoriellement proches de la question\n",
    "relevant_chunks = retriever.invoke(query)\n",
    "\n",
    "# Optionnel : affichage manuel des chunks retrouv√©s (utile pour debug ou v√©rification)\n",
    "# for i, chunk in enumerate(relevant_chunks, 1):\n",
    "#     print(f\"Chunk {i}:\\n{chunk.page_content}\\n\")\n",
    "\n",
    "# Construction du message d'entr√©e √† envoyer au mod√®le\n",
    "# Nous incluons la question et le contenu des documents pour contraindre le LLM √† ne r√©pondre qu'en s'appuyant sur ces sources\n",
    "input_message = (\n",
    "    \"Voici des documents qui vont t'aider √† r√©pondre √† la question : \"\n",
    "    + query\n",
    "    + \"\\n\\nDocuments pertinents : \\n\"\n",
    "    + \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "    + \"\\n\\nDonne une r√©ponse bas√©e uniquement sur les documents qui te sont fournis.\"\n",
    ")\n",
    "\n",
    "# Construction du message complet pour le LLM, avec un r√¥le syst√®me et un message utilisateur\n",
    "messages = [\n",
    "    SystemMessage(content=\"Tu es un assistant qui aide √† retrouver tout type d'informations interne √† une entreprise\"),\n",
    "    HumanMessage(content=input_message)\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c362eac",
   "metadata": {},
   "source": [
    "### üß© Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ccbc1",
   "metadata": {},
   "source": [
    "La soci√©t√© NovTech g√®re de nombreux documents internes :\n",
    "- des rapports d‚Äôincidents (panne, erreur technique, post-mortem),\n",
    "- des proc√©dures op√©rationnelles (onboarding, acc√®s syst√®me, d√©ploiement‚Ä¶).\n",
    "\n",
    "Actuellement, les √©quipes perdent du temps √† chercher les bonnes informations √† travers des fichiers √©parpill√©s.\n",
    "\n",
    "Votre objectif est de construire un assistant bas√© sur l'architecture RAG qui permettra :\n",
    "- de retrouver rapidement les proc√©dures en cas de besoin,\n",
    "- de consulter les r√©solutions d‚Äôincidents similaires,\n",
    "- de r√©pondre √† des questions en langage naturel en s‚Äôappuyant uniquement sur les documents internes.\n",
    "\n",
    "Pour vous aider, vous pouvez suivre les √©tapes suivantes :\n",
    "1. Chargement des documents\n",
    "2. D√©coupage en chunks\n",
    "3. Indexation vectorielle\n",
    "4. Recherche contextuelle\n",
    "5. G√©n√©ration de r√©ponse\n",
    "\n",
    "‚ÑπÔ∏è Les documents de l'entreprise se trouve dans le dossier `data/novtech`.  \n",
    "üí™üèª **Bonus** : Rendre possible un filtrage par cat√©gorie dans les recherches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49ae5cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a000884",
   "metadata": {},
   "source": [
    "# 3. RAG conversationnel\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843e3e1",
   "metadata": {},
   "source": [
    "Dans un cadre d‚Äô**interaction continue**, les utilisateurs posent souvent des questions implicites ou r√©f√©rentielles (ex. ‚ÄúEt lui ?‚Äù). Le **RAG conversationnel** ajoute une √©tape cl√© : la **reformulation de la question en prenant en compte l‚Äôhistorique du dialogue**.  \n",
    "\n",
    "Cette version de RAG permet de maintenir la pertinence des recherches dans la base vectorielle tout en conservant la fluidit√© de la conversation, ce qui la rend adapt√©e aux assistants IA ou aux chatbots avanc√©s.\n",
    "\n",
    "**Exemple**\n",
    "\n",
    "Historique de la conversation :\n",
    "- Utilisateur : *Qui est le CEO de Tesla ?*\n",
    "- IA : *Elon Musk est le CEO de Tesla*.\n",
    "- Utilisateur : *Et de SpaceX ?*\n",
    "\n",
    "‚û°Ô∏è La question ‚ÄúEt de SpaceX ?‚Äù est ambigu√´ seule. Le moteur de recherche (retriever) ne sait pas de quoi il s‚Äôagit exactement.\n",
    "\n",
    "Avec une reformulation de la question de l'utilisateur cela donnerait : ‚ÄúQui est le CEO de SpaceX ?‚Äù\n",
    "\n",
    "‚û°Ô∏è R√©sultat : la requ√™te est claire, et la recherche dans la base vectorielle peut retourner les bons documents.\n",
    "\n",
    "**üëç LangChain facilite ce processus**\n",
    "\n",
    "LangChain fournit une abstraction pr√™te √† l‚Äôemploi gr√¢ce √† la classe `ConversationalRetrievalChain`.\n",
    "Cette classe prend automatiquement en charge :\n",
    "- la reformulation de la question via le LLM\n",
    "- la recherche dans la base vectorielle\n",
    "- la g√©n√©ration de la r√©ponse finale √† partir des documents r√©cup√©r√©s et de l‚Äôhistorique\n",
    "\n",
    "‚û°Ô∏è Elle encapsule ainsi toute la logique conversationnelle d‚Äôun RAG en une seule ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80b0361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Vous :** quit"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de la conversation.\n"
     ]
    }
   ],
   "source": [
    "# Cha√Æne RAG avec historique\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=db.as_retriever())\n",
    "\n",
    "# Boucle de chat\n",
    "chat_history = []\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    clear_output(wait=True)                         # Efface l'affichage pr√©c√©dent\n",
    "    display(Markdown(f\"**Vous :** {user_input}\"))   # Affiche la requ√™te de l'utilisateur\n",
    "\n",
    "    if user_input.lower() in [\"stop\", \"exit\", \"quit\"]:\n",
    "        print(\"Fin de la conversation.\")\n",
    "        break\n",
    "\n",
    "    result = qa_chain({\"question\": user_input, \"chat_history\": chat_history})\n",
    "    display(Markdown(result[\"answer\"]))\n",
    "    chat_history.append((user_input, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a604a4",
   "metadata": {},
   "source": [
    "### üß© Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3873b2",
   "metadata": {},
   "source": [
    "Repartez de l'exercice pr√©c√©dent (NovTech), et impl√©mentez un assistant de conversation continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
